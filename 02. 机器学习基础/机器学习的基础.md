
## 1. 机器学习世界的数据

1. 数据整体叫数据集（data set）
2. 每一行数据称为一个样本
3. 每一列表达样本的一个特征
4. 特征向量、特征空间
5. 分类任务本质就是在特征空间切分
6. 高维空间降维
7. 图像，每一个像素点都是特征

---

## 2. 机器学习的基本任务

1. 分类任务
    1. 二分类 （只有两种结果）
    2. 多分类 （数字识别、图像识别、风险评级）
    3. 多标签分类
2. 回归任务（结果是一个连续数字的值，而非一个类别）

一些情况下，回归任务可以简化成分类任务

机器学习的过程

输入大量学习资料 机器学习算法 模型 输出结果

---

## 3. 监督学习、非监督学习、半监督学习和增强学习

1. 监督学习：给机器的训练数据集拥有“标记“或者”答案”
    1. k近邻
    2. 
2. 非监督学习（给机器的训练数据没有标记或者答案）
    1. 对没有“标记”的数据进行分类 - 聚类分析
    2. 对数据进行降维处理
        1. 特征提取：舍弃不需要的特征
        2. 特征压缩：PCA
        3. 降维的意义：方便可视化
    3. 异常检测
3. 半监督学习 （一部分数据有标记或答案，另一部分数据没有）
    1. 更常见：各种原因产生的标记缺失
    2. 非监督学习 + 监督学习
4. 增强学习 （根据周围环境的情况，采取行动，根据采取行动的结果，学习行动方式）
    1. 无人驾驶
    2. 机器人

## 4. 批量学习、在线学习、参数学习和非参数学习

1. 批量学习（离线学习）
    1. 优点：简单
    2. 问题：如何适应环境变化？解决方案：定时重新批量学习
    3. 缺点：每次重新批量学习，运算量巨大；在某些环境变化非常快的情况下，甚至是不可能的
2. 在线学习
    1. 优点：及时反映新的环境变化
    2. 问题：新的数据带来不好的变化？解决方案：需要加强对数据进行监控
    3. 其他：适用于数据量巨大，完全无法批量学习的环境

---

1. 参数学习（一旦学到了参数，就不再需要原有的数据集）
2. 非参数学习（不对模型进行过多假设；非参数不等于没参数）

---

## 5. 和机器学习相关的“哲学”思考

2001年，微软的论文。

随着数据规模的扩大，准确率提高。

数据即算法？

数据驱动

算法为王？

奥卡姆剃刀：简单的就是好的

没有免费的午餐定理：任意两个算法，他们的期望性能是相同的！

具体到某个特定问题，有些算法可能更好

但没有一种算法绝对比另一个算法好

脱离具体问题，谈算法是没有意义的

## 6. 课程环境搭建

1. anaconda 安装包
2. pycharm

---